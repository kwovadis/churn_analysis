{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f248ab25",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "#### A brief analysis of the churn rate of the telecomunications **Telco Company** ([Link to data](https://www.kaggle.com/blastchar/telco-customer-churn)) using the **CRISP-DM** process: \n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Data Preparation\n",
    "4. Modeling\n",
    "5. Evaluation\n",
    "6. Deployment\n",
    "\n",
    "Main objectives: \n",
    "- Better understand the CRISP-DM process, specifically focusing on:\n",
    "    - Investigating the dataset & setting up business questions\n",
    "    - Preparing & cleaning up data \n",
    "    - Modeling & evaluating the dataset\n",
    "- Become familiarized with simple churn analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn import linear_model # LinearRegression\n",
    "from sklearn import model_selection # train_test_split\n",
    "from sklearn import metrics # r2_score, mean_squared_error\n",
    "# Import required modules\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "cwd = os.getcwd()\n",
    "ls = os.listdir()\n",
    "print(cwd)\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1a75b",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "df = pd.read_csv('.\\data\\WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# Dataset overview\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2635225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows & columns\n",
    "rows = df.shape[0]\n",
    "cols = df.shape[1]\n",
    "print(f'Number of rows = {rows} & number of columns = {cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1a75b",
   "metadata": {},
   "source": [
    "### Business questions to be analyzed in notebook\n",
    "1. What are the total & monthly charges of current vs churned customers and is there a correlation between charges & churning? \n",
    "2. What is the tenure of current vs churned customers? How does tenure correlate with monthly / total charges (tornado plot)?\n",
    "3. What services do people that are customers prefer vs people that churn? (nice tornado plot?)\n",
    "4. Can we predict the churn rate based on the quantitative variables & how does the prediction improve when we add the categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8227066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dtypes of variables\n",
    "df.dtypes.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TotalCharges\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561bab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --> Will give an ERROR!!!!\n",
    "# Convert TotalCharges column to numeric variable \n",
    "pd.to_numeric(df['TotalCharges'], errors = 'raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b30cde",
   "metadata": {},
   "source": [
    "#### Clean up table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of null or empty strings per variable\n",
    "# Clean up data-set\n",
    "# Nans\n",
    "total_null_count = 0\n",
    "index_null_values = []\n",
    "\n",
    "#Check how many empty & blank strings in table\n",
    "df_objects = df.select_dtypes(include=['object']).copy()\n",
    "for col in df.columns:\n",
    "    # print(col)\n",
    "    if col in df_objects.columns:\n",
    "        # Search for empty strings & blank with regex expression r\"^\\s*$\" in categorical variables\n",
    "        col_null_count = df[df[col].str.contains(r\"^\\s*$\")==True][col].count()\n",
    "        col_index_null_values = df[df[col].str.contains(r\"^\\s*$\")==True].index.tolist()\n",
    "        # print(str(col) + \"\\t\" + str(df[col].dtypes) + \"\\t\" + str(col_null_count))\n",
    "        total_null_count += col_null_count\n",
    "        index_null_values.extend(col_index_null_values)\n",
    "        \n",
    "    else: \n",
    "        # search for NaNs in numeric variabls\n",
    "        col_null_count = df[col].isna().sum()\n",
    "        col_index_null_values = df[np.isnan(df[col])].index.tolist()\n",
    "        # print(str(col) + \"\\t\" + str(df[col].dtypes) + \"\\t\" + str(col_null_count))\n",
    "        total_null_count += col_null_count\n",
    "        index_null_values.extend(col_index_null_values)\n",
    "\n",
    "print(f'Number of nan or empty values in table = {total_null_count}')\n",
    "print(f'Nan or empty table rows: {index_null_values}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3fee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean rows that contain Nan or empty values (in total charges)\n",
    "df_clean = df.drop(index_null_values, inplace=False).copy()\n",
    "df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors = 'raise')\n",
    "\n",
    "# pd.to_numeric(df_clean['TotalCharges'], errors = 'raise')\n",
    "\n",
    "\n",
    "rows_clean = df_clean.shape[0]\n",
    "cols_clean = df_clean.shape[1]\n",
    "\n",
    "print(f'New number of rows = {rows_clean} vs old number of rows = {rows}')\n",
    "print(f'New number of cols = {cols_clean} vs old number of cols = {cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658dfbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_customer = df_clean[df_clean.Churn == 'No'] # Subset df_clean to only those that are currently customers\n",
    "churned_customer = df_clean[df_clean.Churn == 'Yes'] # Subset df_clean to only those that have churned\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(list(pd.unique(df_clean[\"Churn\"])),[current_customer.shape[0], churned_customer.shape[0]], label=[current_customer.shape[0], churned_customer.shape[0]])\n",
    "ax.legend()\n",
    "ax.set_xlabel('Churned'), ax.set_ylabel('Count'), ax.set_title('Count of churned customers')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad3113",
   "metadata": {},
   "source": [
    "# Analyze continuous data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19959722",
   "metadata": {},
   "source": [
    "##### What are the statistics of monthly & total charges of customers vs people that churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2940b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_clean.corr(), annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e0948",
   "metadata": {},
   "source": [
    "### Total charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea27749",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_charges_customer = df_clean[df_clean.Churn == 'No']['TotalCharges']\n",
    "total_charges_churned = df_clean[df_clean.Churn == 'Yes']['TotalCharges']\n",
    "\n",
    "print(f'Unique values in churned variable: {pd.unique(df_clean[\"Churn\"])}')\n",
    "print(f'Number of customers: {total_charges_customer.shape[0]}')\n",
    "print(f'Number churned: {total_charges_churned.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_labels = total_charges_customer.describe()[-4:-1].index\n",
    "quantiles_customer = total_charges_customer.describe()[-4:-1]\n",
    "quantiles_churned = total_charges_churned.describe()[-4:-1]\n",
    "\n",
    "# Create a table for quantiles & associated values\n",
    "quantiles_customer_and_churned_total = pd.concat([quantiles_customer, quantiles_churned], axis=1)\n",
    "ratio_total = quantiles_customer / quantiles_churned\n",
    "quantiles_customer_and_churned_total = pd.concat([quantiles_customer_and_churned_total, ratio_total], axis=1)\n",
    "quantiles_customer_and_churned_total.columns = ['Customers', 'Churned', 'Customer vs Churned ratio'] \n",
    "quantiles_customer_and_churned_total = quantiles_customer_and_churned_total.style.set_caption(\"Quantiles of total charges of currnet & churned customers\")\n",
    "quantiles_customer_and_churned_total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ebbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "# PLOTS\n",
    "# Histogram\n",
    "# total_charges_customer.plot(kind = \"hist\", color = 'grey', alpha=0.5, density = True, bins = 15, label='customers', ax=ax) # change density to true, because KDE uses density\n",
    "# total_charges_churned.plot(kind = \"hist\", color = 'salmon', alpha=0.5, density = True, bins = 15, label='churned', ax=ax) # change density to true, because KDE uses density\n",
    "\n",
    "# KDE\n",
    "total_charges_customer.plot(kind = \"kde\", color='black', label='customers', ax=ax)\n",
    "total_charges_churned.plot(kind = \"kde\", color='red', label='churned', ax=ax)\n",
    "\n",
    "# Plot percentiles\n",
    "# total_charges_customer.plot(kind = \"kde\", color='black', label='TotalCharges')\n",
    "# total_charges_churned.plot(kind = \"kde\", color='red', label='TotalCharges')\n",
    "trans = ax.get_xaxis_transform()\n",
    "\n",
    "for i in range(3):\n",
    "    ax.axvline(quantiles_customer[i], color='black', ymax=0.5, alpha = 1, linestyle = \":\")\n",
    "    ax.axvline(total_charges_churned.describe()[-4:-1][i], color='red', ymax=0.75, alpha = 1, linestyle = \":\")\n",
    "    plt.text(quantiles_customer[i], .52, str(quantiles_customer[i]) + '\\n(' + str(quantile_labels[i]) + ')', color = 'black', transform=trans, ha='center', va='bottom')\n",
    "    plt.text(quantiles_churned[i], .77, str(total_charges_churned.describe()[-4:-1][i]) + '\\n(' + str(quantile_labels[i]) + ')', color = 'red', transform=trans, ha='center', va='bottom')\n",
    "\n",
    "# X & Y labels & title\n",
    "ax.set_xlabel(\"Charges ($)\"), ax.set_ylabel('Frequency'), ax.set_title(\"Total charges of current vs churned customers\")\n",
    "# Configure plot\n",
    "ax.set_xlim(0, 10000)\n",
    "ax.set_ylim(0, 0.001)\n",
    "# ax.tick_params(left = False, bottom = False)\n",
    "ax.set_yticks([])\n",
    "ax.spines[['left', 'right', 'top']].set_visible(False)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d402d596",
   "metadata": {},
   "source": [
    "> This result shows that the **current customers have higher total charges than the churned customers**, with the P50 being ~2 times larger for the current customers in comparison to the churned customers. It would be worthwhile to investigate what causes them to have higher total charges. 1st hypothesis that comes to mind is having a longer tenure. If so, it would be important to answer what makes them stay longer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e0948",
   "metadata": {},
   "source": [
    "### Monthly charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea56a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data\n",
    "# ALL customers - split data set into customers vs churned \n",
    "monthly_charges_customer = df_clean[df_clean.Churn == 'No']['MonthlyCharges']\n",
    "monthly_charges_churned = df_clean[df_clean.Churn == 'Yes']['MonthlyCharges']\n",
    "\n",
    "print(f'Number of customers: {monthly_charges_customer.shape[0]}')\n",
    "print(f'Number churned customers: {monthly_charges_churned.shape[0]}')\n",
    "\n",
    "# OLD cusomers - split data set into old customers vs old churned (without new customers where TotalCharges = 0)\n",
    "monthly_charges_customer_old = df_clean[(df_clean.Churn == 'No') & (df_clean.TotalCharges != 0)]['MonthlyCharges'] \n",
    "monthly_charges_churned_old = df_clean[(df_clean.Churn == 'Yes') & (df_clean.TotalCharges != 0)]['MonthlyCharges']\n",
    "\n",
    "# print(f'Number of old customers: {monthly_charges_customer_old.shape[0]}')\n",
    "# print(f'Number of old churned customers: {monthly_charges_churned_old.shape[0]}')\n",
    "\n",
    "print(f'Number of old customers (where total charges > 0) = total number of customers: {monthly_charges_customer_old.shape[0] == monthly_charges_customer.shape[0]}')\n",
    "\n",
    "# TODO:\n",
    "# --> plot histograms as line plot: https://towardsdatascience.com/take-your-histograms-to-the-next-level-using-matplotlib-5f093ad7b9d3\n",
    "# --> compare average / p10 / p90 percentiles + skeweness\n",
    "# --> What if we take away the customers that just joined (i.e. have no total charges)?\n",
    "# --> Is there a clear difference in the monthly & total charges between people that stay vs people that churned? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951c61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_labels = monthly_charges_customer.describe()[-4:-1].index\n",
    "quantiles_customer_monthly = monthly_charges_customer.describe()[-4:-1]\n",
    "quantiles_churned_monthly = monthly_charges_churned.describe()[-4:-1]\n",
    "\n",
    "# Compare statistics of all vs old customers\n",
    "ratio_all_monthly = quantiles_customer_monthly / quantiles_churned_monthly\n",
    "quantiles_customer_and_churned_monthly = pd.concat([quantiles_customer_monthly, quantiles_churned_monthly, ratio_all_monthly], axis=1)\n",
    "quantiles_customer_and_churned_monthly.columns = ['Customers', 'Churned', 'Customers vs churned ratio'] \n",
    "quantiles_customer_and_churned_monthly = quantiles_customer_and_churned_monthly.style.set_caption(\"Quantiles of monthly charges of current & churned customers\")\n",
    "quantiles_customer_and_churned_monthly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a70f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (16,6))\n",
    "\n",
    "# FIGURE 1\n",
    "# PLOTS\n",
    "# KDE\n",
    "monthly_charges_customer.plot(kind = \"kde\", color='black', label='Customers', ax=ax)\n",
    "monthly_charges_churned.plot(kind = \"kde\", color='red', label='Churned', ax=ax)\n",
    "\n",
    "# Plot percentiles\n",
    "# monthly_charges_customer.plot(kind = \"kde\", color='black', label='MonthlyCharges')\n",
    "# monthly_charges_churned.plot(kind = \"kde\", color='red', label='MonthlyCharges')\n",
    "trans = ax.get_xaxis_transform()\n",
    "\n",
    "for i in range(3):\n",
    "    ax.axvline(quantiles_customer_monthly[i], color='black', ymax=0.5, alpha = 1, linestyle = \":\")\n",
    "    ax.axvline(quantiles_churned_monthly[i], color='red', ymax=0.75, alpha = 1, linestyle = \":\")\n",
    "    plt.text(quantiles_customer_monthly[i], .52, str(quantiles_customer_monthly[i]) + '\\n(' + str(quantile_labels[i]) + ')', color = 'black', transform=trans, ha='center', va='bottom')\n",
    "    plt.text(quantiles_churned_monthly[i], .77, str(quantiles_churned_monthly[i]) + '\\n(' + str(quantile_labels[i])+ ')', color = 'red', transform=trans, ha='center', va='bottom')\n",
    "\n",
    "# X & Y labels & title\n",
    "ax.set_xlabel(\"Charges ($)\"), ax.set_ylabel('Frequency'), ax.set_title(\"Monthly charges of current vs churned customers\")\n",
    "# Configure plot\n",
    "ax.set_xlim(0, 140), ax.set_ylim(0, 0.05)\n",
    "# ax.tick_params(left = False, bottom = False)\n",
    "ax.set_yticks([])\n",
    "ax.spines[['left', 'right', 'top']].set_visible(False)\n",
    "ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2ed3e8",
   "metadata": {},
   "source": [
    "> This result shows that the **current customers have lower monthly charges than the churned customers**, with the P25/P50/P75 all being lower for the current customers in comparison to the churned customers. It is interesting that while the total charges for current customers are higher, the monthly charges for the current customers are lower. One reasonable explanation for this would be a longer tenure for current customers that have lower monthly charges. Moreover, after 60$ the churn rate seems to increase substantially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249325e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create series of tenure of current & churned customers\n",
    "tenure_customer = df_clean[df_clean.Churn == 'No']['tenure']#.value_counts(normalize=True, bins = [0.001,1,2,3,4,5,10,15,20,30,40,50,60], sort=False)\n",
    "tenure_churned = df_clean[df_clean.Churn == 'Yes']['tenure']#.value_counts(normalize=True, bins = [0.001,1,2,3,4,5,10,15,20,30,40,50,60], sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfcc6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_variable_statistics_current_vs_churned(current, churned, columns = ['Current', 'Churned', 'Current vs churned ratio'], title='Title'):\n",
    "    #Get quantiles\n",
    "    quantile_labels = current.describe()[-4:-1].index\n",
    "    quantiles_current = current.describe()[-4:-1]\n",
    "    quantiles_churned = churned.describe()[-4:-1]\n",
    "    # Compare statistics of all vs old customers\n",
    "    ratio = quantiles_current / quantiles_churned\n",
    "    quantile_values = pd.concat([quantiles_current, quantiles_churned, ratio], axis=1)\n",
    "    quantile_values.columns = columns\n",
    "    quantiles_current_and_churned = quantile_values.style.set_caption(title)\n",
    "    display(quantiles_current_and_churned)\n",
    "    return quantile_labels, quantile_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_labels, quantile_values = merged_variable_statistics_current_vs_churned(tenure_customer, tenure_churned, title='Quantiles of tenure of current & churned customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (16,6))\n",
    "# FIGURE 1\n",
    "# PLOTS\n",
    "# KDE\n",
    "tenure_customer.plot(kind = \"kde\", color='black', label='Customers', ax=ax)\n",
    "tenure_churned.plot(kind = \"kde\", color='red', label='Churned', ax=ax)\n",
    "\n",
    "# # Plot percentiles\n",
    "trans = ax.get_xaxis_transform()\n",
    "ax.axvline(quantile_values['Current']['50%'], color='black', ymax=0.5, alpha = 1, linestyle = \":\")\n",
    "ax.axvline(quantile_values['Churned']['50%'], color='red', ymax=0.75, alpha = 1, linestyle = \":\")\n",
    "plt.text(quantile_values['Current']['50%'], .52, str(quantile_values['Current']['50%']) +  '\\n(' + str(quantile_labels[1]) + ')', color = 'black', transform=trans, ha='center', va='bottom')\n",
    "plt.text(quantile_values['Churned']['50%'], .77, str(quantile_values['Churned']['50%']) +  '\\n(' + str(quantile_labels[1]) + ')', color = 'red', transform=trans, ha='center', va='bottom')\n",
    "\n",
    "# X & Y labels & title\n",
    "ax.set_xlabel(\"Tenure (Months)\"), ax.set_ylabel('Frequency'), ax.set_title(\"Tenure of current vs churned customers\")\n",
    "# Configure plot\n",
    "ax.set_xlim(0, 90), ax.set_ylim(0, 0.05)\n",
    "ax.set_yticks([])\n",
    "ax.spines[['left', 'right', 'top']].set_visible(False)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac812b68",
   "metadata": {},
   "source": [
    "> This result shows that the **current customers have longer tenure than the churned customers**, with the P25/P50/P75 all being higher for the current customers in comparison to the churned customers. This is as expected, since the current customers had larger total charges, while maintaining lower monthly charges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7871062f",
   "metadata": {},
   "source": [
    "# Analyze categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a46c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical data: \", list(df_clean.select_dtypes(include=['object']).columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', \n",
    "'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', \n",
    "'Contract', 'PaperlessBilling', 'PaymentMethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categories\")\n",
    "for col in df_clean.columns:\n",
    "       if col in categories:\n",
    "              print(col, '\\t', df_clean[col].unique(), '\\t length: ', len(df_clean[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa0a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# floor()\n",
    "int(np.ceil(len(categories) / 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_analysis_plots(df_clean, categories):\n",
    "    # fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))\n",
    "\n",
    "    fig, axes = plt.subplots(int(np.ceil(len(categories) / 3)), 3, figsize=(16, 16))\n",
    "    fig.suptitle('Categorical data')\n",
    "    i_idx = 0\n",
    "    j_idx = 0\n",
    "    total_idx = 0\n",
    "    for cat in categories:\n",
    "        # print(f'i = {i_idx}, j = {j_idx}')\n",
    "        # ax = sns.catplot(ax = axes[i_idx, j_idx], x=cat, hue=\"Churn\", kind=\"count\", legend_out=False, data=df_clean).set(title=f'Current vs churned customers based on {cat}')\n",
    "        sns.countplot(ax = axes[i_idx, j_idx], x=cat, hue=\"Churn\", data=df_clean).set(title=f'{cat}')\n",
    "        if len(df_clean[cat].unique()) > 3:\n",
    "            axes[i_idx, j_idx].set_xticklabels(axes[i_idx, j_idx].get_xticklabels(), rotation=40, ha=\"right\")\n",
    "        # axes[i_idx, j_idx].set_xticklabels(axes[i_idx, j_idx].get_xticklabels(), rotation=40, ha=\"right\")\n",
    "        #  = sns.countplot(x=\"Column\", data=ds)\n",
    "\n",
    "        # _ = ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "        # if len(df_clean[cat].unique()) > 2:\n",
    "        #     locs, labels = plt.xticks()\n",
    "        #     plt.setp(labels, rotation=45)\n",
    "        # plt.legend(loc=\"upper right\")\n",
    "        j_idx += 1\n",
    "        if j_idx > 2:\n",
    "            j_idx = 0\n",
    "            i_idx += 1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b764f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_analysis_plots(df_clean, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function taken from UDACITY data science class\n",
    "def total_count(df, col1, col2, look_for):\n",
    "    new_df = defaultdict(int)\n",
    "    #loop through list of ed types\n",
    "    for val in look_for:\n",
    "        #loop through rows\n",
    "        for idx in range(df.shape[0]):\n",
    "            #if the ed type is in the row add 1\n",
    "            if val in df[col1][idx]:\n",
    "                new_df[val] += int(df[col2][idx])\n",
    "    new_df = pd.DataFrame(pd.Series(new_df)).reset_index()\n",
    "    new_df.columns = [col1, col2]\n",
    "    new_df.sort_values('count', ascending=False, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "possible_vals = list(df_clean['Contract'].unique())\n",
    "\n",
    "def clean_and_plot(df, title='Method of Educating Suggested', plot=True):\n",
    "    # Plot bar graph of different contracts (for current & churned customers)\n",
    "    x = df['Contract'].value_counts().reset_index()\n",
    "    x.rename(columns={'index': 'contract', 'Contract': 'count'}, inplace=True)\n",
    "    x_df = total_count(x, 'contract', 'count', possible_vals)\n",
    "\n",
    "    x_df.set_index('contract', inplace=True)\n",
    "    if plot:\n",
    "        (x_df/x_df.sum()).plot(kind='bar', legend=None)\n",
    "        plt.title('Contract type')\n",
    "        plt.show()\n",
    "    props_x_df = x_df/x_df.sum()\n",
    "    return props_x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6853b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_customer_perc = clean_and_plot(current_customer, 'Customer', plot=False)\n",
    "churned_customer_perc = clean_and_plot(churned_customer, 'Churned', plot=False)\n",
    "\n",
    "comp_df = pd.merge(current_customer_perc, churned_customer_perc, left_index=True, right_index=True)\n",
    "comp_df.columns = ['current_customer_perc', 'churned_customer_perc']\n",
    "comp_df['Diff_Contract_Type'] = comp_df['current_customer_perc'] - comp_df['churned_customer_perc']\n",
    "comp_df.style.bar(subset=['Diff_Contract_Type'], align='mid', color=['#d65f5f', '#5fba7d'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2008d",
   "metadata": {},
   "source": [
    "> **Month-to-month** contracts are the most popular type of contracts both for current and churned customers. For customers that churned, more than 88% of their contracts are month-to-month, which is compared to c. 43% of current customers which are currently on that type of contract. \n",
    "\n",
    "> **Long-term contracts** that are equal to or are longer than a year account for a little over 10% of customers that have churned, whereas for current customers, long-term contracts account for over 55% of all of their contracts. \n",
    "\n",
    "> It appears that customers with short-term month-to-month contracts are generally more likely to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180bca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave only categorical data\n",
    "\n",
    "obj_df = df_clean.select_dtypes(include=['object']).copy()\n",
    "new_obj_df = obj_df.drop(columns = 'customerID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69649e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_df(df, cat_cols, dummy_na): \n",
    "    for col in  cat_cols: \n",
    "        try: \n",
    "            # for each cat add dummy var, drop original column \n",
    "            df = pd.concat([df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=dummy_na)], axis=1) \n",
    "        except: \n",
    "            continue \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols_lst = new_obj_df.columns \n",
    "df_cat = create_dummy_df(new_obj_df, obj_cols_lst, dummy_na=False)\n",
    "print(f'shape of df_cat: {df_cat.shape}')\n",
    "print(f'columns: {df_cat.columns}')\n",
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_cat.corr(), fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ef9be",
   "metadata": {},
   "source": [
    "# Train prediction model\n",
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_mod(df, response_col, cat_cols, dummy_na, test_size, rand_state=42):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - a dataframe holding all the variables of interest\n",
    "    response_col - a string holding the name of the column \n",
    "    cat_cols - list of strings that are associated with names of the categorical columns\n",
    "    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n",
    "    test_size - a float between [0,1] about what proportion of data should be in the test dataset\n",
    "    rand_state - an int that is provided as the random state for splitting the data into training and test \n",
    "    \n",
    "    OUTPUT:\n",
    "    test_score - float - r2 score on the test data\n",
    "    train_score - float - r2 score on the test data\n",
    "    lm_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    \n",
    "    The function does:\n",
    "    1. Split your data into an X matrix and a response vector y\n",
    "    2. Create training and test sets of data\n",
    "    3. Instantiate a LinearRegression model with normalized data\n",
    "    3. Fit your model to the training data\n",
    "    4. Predict the response for the training data and the test data\n",
    "    5. Obtain an rsquared value for both the training and test data\n",
    "    '''\n",
    "    \n",
    "    X = df.drop(response_col, axis=1) #5 \n",
    "    y = df[response_col]\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = test_size, random_state=rand_state) #6\n",
    "    lm_model = linear_model.LinearRegression(normalize=True) #7\n",
    "    lm_model.fit(X_train, y_train) #8\n",
    "    y_test_preds = lm_model.predict(X_test) #9\n",
    "    y_train_preds = lm_model.predict(X_train)\n",
    "    test_score = metrics.r2_score(y_test, y_test_preds) #10\n",
    "    train_score = metrics.r2_score(y_train, y_train_preds)\n",
    "\n",
    "    return test_score, train_score, lm_model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_cat_linear_mod, train_score_cat_linear_mod, lm_model_cat, X_train_cat, X_test_cat, y_train_cat, y_test_cat = fit_linear_mod(\n",
    "    df_cat, 'Churn_Yes', obj_cols_lst, dummy_na=False, test_size=.2, rand_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Using only categorical data, we get a train r2 score of: \\n{train_score_cat_linear_mod} \\nand a test r2 score of: \\n{test_score_cat_linear_mod}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d295ca6",
   "metadata": {},
   "source": [
    "# Normalize - continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90115ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize a column\n",
    "def normalize_cont_columns(df):\n",
    "    df_num = df.select_dtypes(exclude=['object'])\n",
    "    # Create a minimum and maximum processor object\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    for col in df_num:\n",
    "        df_num[col] = min_max_scaler.fit_transform(df_num[[col]].values.astype(float))\n",
    "\n",
    "    return df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = normalize_cont_columns(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cat_and_num = pd.concat([df_cat, df_clean.select_dtypes(exclude=['object'])], axis=1)\n",
    "df_cat_and_num = pd.concat([df_cat, df_num], axis=1)\n",
    "df_cat_and_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22248010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_and_num.corr()['Churn_Yes'].sort_values(ascending=False).plot(kind='bar', figsize=(20,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_norm_linear_mod, train_score_norm_linear_mod, lm_model, X_train, X_test, y_train, y_test = fit_linear_mod(\n",
    "    df_cat_and_num, 'Churn_Yes', obj_cols_lst, dummy_na=False, test_size=.2, rand_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Using both categorical & continuous data, we get a train r2 score of: \\n{train_score_norm_linear_mod} vs {train_score_cat_linear_mod} \\nand a test r2 score of: \\n{test_score_norm_linear_mod} vs {test_score_cat_linear_mod}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_weights(coefficients, X_train):\n",
    "    '''\n",
    "    INPUT:\n",
    "    coefficients - the coefficients of the linear model \n",
    "    X_train - the training data, so the column names can be used\n",
    "    OUTPUT:\n",
    "    coefs_df - a dataframe holding the coefficient, estimate, and abs(estimate)\n",
    "    \n",
    "    Provides a dataframe that can be used to understand the most influential coefficients\n",
    "    in a linear model by providing the coefficient estimates along with the name of the \n",
    "    variable attached to the coefficient.\n",
    "    '''\n",
    "    coefs_df = pd.DataFrame()\n",
    "    coefs_df['est_int'] = X_train.columns\n",
    "    coefs_df['coefs'] = coefficients\n",
    "    coefs_df['abs_coefs'] = np.abs(coefficients)\n",
    "    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n",
    "    return coefs_df\n",
    "\n",
    "#Use the function\n",
    "coef_df = coef_weights(lm_model.coef_, X_train)\n",
    "\n",
    "#A quick look at the top results\n",
    "coef_df#.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c78a90",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e599f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_weights(X_df, classifier, classifier_name):\n",
    "    weights = pd.Series(classifier.coef_[0], index=X_df.columns.values).sort_values(ascending=False)\n",
    "\n",
    "    top_weights_selected = weights[:10]\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.tick_params(labelsize=10)\n",
    "    plt.title(f'{classifier_name} - Top 10 Features')\n",
    "    top_weights_selected.plot(kind='bar')\n",
    "    \n",
    "    bottom_weights_selected = weights[-10:]\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.tick_params(labelsize=10)\n",
    "    plt.title(f'{classifier_name} - Bottom 10 Features')\n",
    "    bottom_weights_selected.plot(kind='bar')\n",
    "\n",
    "    return print(\"\")\n",
    "\n",
    "def confusion_matrix_plot(X_train, y_train, X_test, y_test, classifier, y_pred, classifier_name):\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    metrics.plot_confusion_matrix(classifier, X_test, y_test, display_labels=[\"No Churn\", \"Churn\"], cmap=plt.cm.Blues, normalize=None, ax=ax)\n",
    "    ax.set_title(f'{classifier_name} - Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    metrics.plot_confusion_matrix(classifier, X_test, y_test, display_labels=[\"No Churn\", \"Churn\"], cmap=plt.cm.Blues, normalize='true', ax=ax)\n",
    "    ax.set_title(f'{classifier_name} - Confusion Matrix (norm.)')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Accuracy score test: {metrics.accuracy_score(y_test, y_pred)}')\n",
    "    print(f'Accuracy score train: {classifier.score(X_train, y_train)} (as comparison)')\n",
    "    return print(\"\")\n",
    "\n",
    "def roc_curve_auc_score(X_test, y_test, y_pred_probabilities, classifier_name):\n",
    "\n",
    "    y_pred_prob = y_pred_probabilities[:, 1]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label=f'{classifier_name}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{classifier_name} - ROC Curve')\n",
    "    plt.show()\n",
    "\n",
    "    return print(f'AUC Score (ROC): {metrics.roc_auc_score(y_test, y_pred_prob)}\\n')\n",
    "\n",
    "def precision_recall_curve_and_scores(X_test, y_test, y_pred, y_pred_probabilities, classifier_name):\n",
    "    y_pred_prob = y_pred_probabilities[:, 1]\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "    plt.plot(recall, precision, label=f'{classifier_name}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{classifier_name} - Precision-Recall Curve')\n",
    "    plt.show()\n",
    "\n",
    "    f1_score_result, auc_score_result = metrics.f1_score(y_test, y_pred), metrics.auc(recall, precision)\n",
    "\n",
    "    return print(f'f1 Score: {f1_score_result} \\n AUC Score (PR): {auc_score_result}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logreg_mod(df, response_col, test_size, rand_state=42):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - a dataframe holding all the variables of interest\n",
    "    response_col - a string holding the name of the column \n",
    "    cat_cols - list of strings that are associated with names of the categorical columns\n",
    "    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n",
    "    test_size - a float between [0,1] about what proportion of data should be in the test dataset\n",
    "    rand_state - an int that is provided as the random state for splitting the data into training and test \n",
    "    \n",
    "    OUTPUT:\n",
    "    test_score - float - r2 score on the test data\n",
    "    train_score - float - r2 score on the test data\n",
    "    lm_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    \n",
    "    The function does:\n",
    "    1. Split your data into an X matrix and a response vector y\n",
    "    2. Create training and test sets of data\n",
    "    3. Instantiate a LinearRegression model with normalized data\n",
    "    3. Fit your model to the training data\n",
    "    4. Predict the response for the training data and the test data\n",
    "    5. Obtain an rsquared value for both the training and test data\n",
    "    '''\n",
    "    \n",
    "    X = df.drop(response_col, axis=1) #5 \n",
    "    y = df[response_col]\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = test_size, random_state=rand_state) #6\n",
    "    lm_model = linear_model.LogisticRegression(random_state=rand_state, max_iter = 100) #7\n",
    "    lm_model.fit(X_train, y_train) #8\n",
    "    y_test_preds = lm_model.predict(X_test) #9\n",
    "    y_train_preds = lm_model.predict(X_train)\n",
    "    test_score = metrics.r2_score(y_test, y_test_preds) #10\n",
    "    train_score = metrics.r2_score(y_train, y_train_preds)\n",
    "\n",
    "    return test_score, train_score, lm_model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_logreg_mod, train_score_logreg_mod, lm_model_logreg, X_train_logreg, X_test_logreg, y_train_logreg, y_test_logreg = fit_logreg_mod(\n",
    "    df_cat_and_num, 'Churn_Yes', .2, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41661a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Using linear model we get a train r2 score of: \\n{train_score_norm_linear_mod} vs logistic regression model {train_score_logreg_mod} \\nand a test r2 score of: \\n{test_score_norm_linear_mod} vs {test_score_logreg_mod}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logreg = lm_model_logreg.predict(X_test)\n",
    "y_pred_logreg_prob = lm_model_logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d645e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df_cat_and_num.drop('Churn_Yes', axis=1)\n",
    "\n",
    "feature_weights(X1, lm_model_logreg, 'Log. Regression')\n",
    "confusion_matrix_plot(X_train_logreg, y_train_logreg, X_test_logreg, y_test_logreg, lm_model_logreg, y_pred_logreg, 'Log. Regression')\n",
    "roc_curve_auc_score(X_test_logreg, y_test_logreg, y_pred_logreg_prob, 'Log. Regression')\n",
    "precision_recall_curve_and_scores(X_test_logreg, y_test_logreg, y_pred_logreg, y_pred_logreg_prob, 'Log. Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62daf53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the function\n",
    "coef_df = coef_weights(lm_model_logreg.coef_[0], X_train_logreg)\n",
    "\n",
    "#A quick look at the top results\n",
    "coef_df#.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8203ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "def fit_knn_mod(df, response_col, test_size=.3, rand_state=42):    \n",
    "    X = df.drop(response_col, axis=1) #5 \n",
    "    y = df[response_col]\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = test_size, random_state=rand_state) #6\n",
    "    lm_model = neighbors.KNeighborsClassifier() #random_state=rand_state, max_iter = 1000) #7\n",
    "    lm_model.fit(X_train, y_train) #8\n",
    "    y_test_preds = lm_model.predict(X_test) #9\n",
    "    y_train_preds = lm_model.predict(X_train)\n",
    "    test_score = metrics.r2_score(y_test, y_test_preds) #10\n",
    "    train_score = metrics.r2_score(y_train, y_train_preds)\n",
    "\n",
    "    return test_score, train_score, lm_model, X_train, X_test, y_train, y_test, y_test_preds, y_train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0fbc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_knn, train_score_knn, lm_model_knn, X_train_knn, X_test_knn, y_train_knn, y_test_knn, y_test_preds_knn, y_train_preds_knn = fit_knn_mod(\n",
    "    df_cat_and_num, 'Churn_Yes', test_size=.3, rand_state=42)\n",
    "\n",
    "print(f'Using linear model we get a train r2 score of: \\n{train_score_knn} \\nand a test r2 score of: \\n{test_score_knn}')\n",
    "# from sklearn \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
